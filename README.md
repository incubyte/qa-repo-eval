# QA Repository Evaluation Tool

An AI-powered tool for evaluating QA practices in software repositories. Perfect for assessing automation test repositories, QA candidate submissions, and code quality analysis.

## ğŸ¯ Overview

This tool provides comprehensive evaluation of QA repositories across four key categories:

- **Test Automation** (30%) - Framework usage, test organization, coverage
- **Technical Skills** (25%) - API/UI testing, design patterns, assertions
- **Quality Process** (25%) - Testing strategy, documentation, collaboration
- **CI Pipeline** (20%) - Automation integration, deployment practices

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd qa-repo-eval-tool

# Install dependencies
uv sync

# Set your OpenAI API key
export OPENAI_API_KEY='your-api-key-here'
```

### Basic Usage

```bash
# Check environment setup
python main.py check

# Evaluate a single repository
python main.py evaluate "https://github.com/user/automation-repo"

# Batch evaluation from file
echo "https://github.com/user/repo1" > repos.txt
echo "https://github.com/user/repo2" >> repos.txt
python main.py batch repos.txt
```

> **Alternative**: You can also use `python -m src.qa_repo_eval_tool.cli <command>` if you prefer the module syntax.

## ğŸ“Š Features

### âœ… **Single Repository Evaluation**

- Comprehensive QA assessment with detailed scoring
- Visual progress tracking and rich console output
- Strengths and improvement recommendations
- Pass/Conditional Pass/Fail verdicts

### âœ… **Batch Processing**

- Evaluate multiple repositories from input file
- Generate JSON, CSV, and summary reports
- Batch analytics and success rates
- Continue-on-error for resilient processing

### âœ… **Professional Reports**

- **JSON** - Complete structured data for analysis
- **CSV** - Spreadsheet-compatible format
- **Summary** - Human-readable statistics and insights

### âœ… **AI-Powered Analysis**

- Framework detection (Selenium, Cypress, pytest, etc.)
- BDD implementation assessment (Cucumber, Gherkin)
- Page Object Model evaluation
- CI/CD pipeline analysis

## ğŸ› ï¸ Use Cases

### **QA Candidate Assessment**

Perfect for evaluating automation test submissions like:

- Selenium WebDriver projects
- Cypress E2E tests
- API automation suites
- BDD/Cucumber implementations

### **Code Quality Review**

- Repository health checks
- Best practices compliance
- Framework usage assessment
- Test coverage analysis

### **Team Evaluation**

- Batch assessment of team repositories
- Standardization compliance
- Training needs identification

## ğŸ“‹ CLI Commands

### Environment Check

```bash
python main.py check
```

Validates API key and dependencies.

### Single Evaluation

```bash
python main.py evaluate "https://github.com/user/repo" [OPTIONS]

Options:
  --shallow/--full     Use shallow clone (faster) [default: shallow]
  --keep-clone         Keep cloned repository for inspection
  --verbose/--quiet    Progress detail level [default: verbose]
```

### Batch Evaluation

```bash
python main.py batch INPUT_FILE [OPTIONS]

Options:
  --output-dir PATH    Output directory [default: qa_reports]
  --shallow/--full     Clone type for all repositories
  --continue/--stop    Continue if a repository fails [default: continue]
  --verbose/--quiet    Progress detail level
```

### Version Info

```bash
python main.py version
```

## ğŸ“ˆ Sample Output

```
ğŸ“Š QA Evaluation Results: https://github.com/user/automation-repo

ğŸ¯ Overall QA Score: 78/100
ğŸ“ˆ QA Level: Advanced
âš–ï¸ Final Verdict: PASS
ğŸ’» Primary Language: java
ğŸ§ª Test Files: 15/28
ğŸ”§ Test Frameworks: selenium, cucumber, testng

ğŸ“Š Category Scores:
Test Automation     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 8.2/10
CI Pipeline         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 6.0/10
Quality Process     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 7.5/10
Technical Skills    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 8.0/10

âœ… Strengths:
  â€¢ Excellent BDD implementation with Cucumber
  â€¢ Strong Page Object Model structure
  â€¢ Good test coverage for core flows

âš ï¸ Areas for Improvement:
  â€¢ Missing CI/CD pipeline configuration
  â€¢ Limited error handling in tests
  â€¢ Could improve test data management
```

## ğŸ—ï¸ Architecture

```
src/qa_repo_eval_tool/
â”œâ”€â”€ cli.py              # Command-line interface
â”œâ”€â”€ metrics.py          # Core evaluation orchestration
â”œâ”€â”€ qa_analysis.py      # AI-powered analysis engine
â”œâ”€â”€ reporter.py         # Report generation (JSON/CSV/text)
â”œâ”€â”€ git_utils.py        # Repository operations
â”œâ”€â”€ types.py            # Data models and metrics
â”œâ”€â”€ metrics_calculator.py # Scoring and verdict logic
â””â”€â”€ utils/
    â””â”€â”€ prompts.py      # AI evaluation prompts
```

## ğŸ”§ Configuration

### Required Environment Variables

```bash
OPENAI_API_KEY=your-openai-api-key
```

### Input File Format

```text
# QA repositories for evaluation
https://github.com/user/selenium-tests
https://github.com/user/api-automation
https://github.com/user/cypress-e2e

# Comments start with #
# Empty lines are ignored
```

## ğŸ¯ Evaluation Criteria

### Test Automation (30%)

- Test coverage and organization
- Framework usage quality
- Assertion patterns
- Test data management

### Technical Skills (25%)

- Test design patterns (Page Object, Builder, etc.)
- API testing implementation
- UI testing practices
- Performance and security considerations

### Quality Process (25%)

- Testing strategy evidence
- Documentation quality
- Collaboration indicators
- Process maturity

### CI Pipeline (20%)

- Pipeline configuration
- Test integration
- Deployment automation
- Environment management

## ğŸ“Š Scoring System

- **Expert (85-100)**: Advanced QA practices, production-ready
- **Advanced (70-84)**: Strong QA skills, minor improvements needed
- **Intermediate (50-69)**: Good foundation, development areas identified
- **Beginner (0-49)**: Basic skills, significant improvement needed

### Verdicts

- **PASS** (â‰¥70): Ready for QA roles
- **CONDITIONAL_PASS** (50-69): Potential with development
- **FAIL** (<50): Needs significant improvement

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your changes
4. Add tests if applicable
5. Submit a pull request
